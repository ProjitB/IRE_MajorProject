{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing the Word2Vec Model, download the file from https://nlp.stanford.edu/projects/glove/\n",
    "# Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip \n",
    "# Unzip the file then run: python3 -m gensim.scripts.glove2word2vec --input  glove.6B.300d.txt --output glove.6B.300d.w2vformat.txt\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/glove.6B.300d.w2vformat.txt')\n",
    "word_freq_map = {}\n",
    "with open(\"vocab\", 'r') as vocab_file:\n",
    "    lines = vocab_file.readlines()\n",
    "    for line in lines:\n",
    "        word_freq_map[line.split()[0]] = int(line.split()[1])\n",
    "    \n",
    "stop_list = sorted(word_freq_map.items(), key=operator.itemgetter(1), reverse=True)[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the full training data and split it into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_and_split(train_corpus_path):\n",
    "    train_data_map = {}\n",
    "    file_no = 0\n",
    "    with open(train_corpus_path, 'r') as train_data_file:\n",
    "        line_count = 0\n",
    "        while file_no < 11:\n",
    "            if line_count < 20000:\n",
    "                line_data = train_data_file.readline()\n",
    "                if line_data:\n",
    "                    line_map = json.loads(line_data)\n",
    "                    article_id = line_map['article_id']\n",
    "                    del line_map['article_id']\n",
    "                    train_data_map[article_id] = line_map\n",
    "                    line_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "                    pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"File \", file_no, \" Done\")\n",
    "                train_data_map.clear()\n",
    "                file_no += 1\n",
    "                line_count = 0\n",
    "        with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "            pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loads a particular pickle file of the training data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(train_corpus_path):\n",
    "    data_path = train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain\"\n",
    "    data_map = {}\n",
    "    with open(data_path + \"/\" + \"train_0.pickle\", 'rb') as handle:\n",
    "        data_map = pickle.load(handle)\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets all the sentences of the article along with its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_metadata(data_map):\n",
    "    full_text = []\n",
    "    sentence_metadata = []\n",
    "    list_of_sentences = []\n",
    "    c = 0\n",
    "    for article_id, data in data_map.items():\n",
    "        if c == 55:\n",
    "            section_data = data['sections']\n",
    "            section_names = data['section_names']\n",
    "            for i, section in enumerate(section_data):\n",
    "                for line in section:\n",
    "                    split_line = line.split('.')\n",
    "                    for l in split_line:\n",
    "                        list_of_sentences.append(l)\n",
    "                        sentence_metadata.append(section_names[i])\n",
    "        c += 1\n",
    "    return list_of_sentences, sentence_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Following 2 functions are used for Preprocessing of a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(word):\n",
    "    \"\"\"\n",
    "    Checks if word is ascii or not\n",
    "    :param word: token\n",
    "    :return: Boolean\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    try:\n",
    "        word = word.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        valid = False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens(sentence):\n",
    "    punc_map = {}\n",
    "    punc_map = punc_map.fromkeys('!\"\\'()*+,;<>[]^`{|}~:=%&_#?-$/', ' ')\n",
    "    table = str.maketrans(punc_map)\n",
    "    tokens = sentence.lower().translate(table).split()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stop_words = list(stop_words) + stop_list\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and is_ascii(word) and '@' not in word and '\\\\' not in word and len(word) > 1]            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets the processed sentences for each sentence of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_sentences(list_of_sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = \" \".join(sentence)\n",
    "        processed_sentences.append(get_processed_tokens(sentence))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gives the number of words common between given 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_of_common_word(sentence1, sentence2):\n",
    "    common_count = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2:\n",
    "            if s1 == s2:\n",
    "                common_count += 1\n",
    "    return common_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vec_sim(sentence1, sentence2):\n",
    "    score = 0\n",
    "    for word1 in sentence1:\n",
    "        for word2 in sentence2:\n",
    "            try:\n",
    "                temp = model.similarity(word1, word2)\n",
    "            except:\n",
    "                temp = 0\n",
    "            score += temp\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic scoring function which gives a score between 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(sentence1, sentence2, metadata):\n",
    "    len_normalize = len(sentence1) + len(sentence2) + 1 # Normalizing by length of vector\n",
    "    common_words = get_no_of_common_word(sentence1, sentence2)\n",
    "    word_vec_score = get_word_vec_sim(sentence1, sentence2)\n",
    "    score = common_words / 2*len_normalize + word_vec_score / len_normalize\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Makes the graph which has relations between every pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(processed_sentences, metadata):\n",
    "    sentence_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    sentence_common_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    \n",
    "    for i in range(len(processed_sentences)):\n",
    "        for j in range(len(processed_sentences)):\n",
    "            sentence1 = processed_sentences[i]\n",
    "            sentence2 = processed_sentences[j]\n",
    "            if i == j:\n",
    "                sentence_graph[i][j] = 0\n",
    "                sentence_common_graph[i][j] = 0\n",
    "            else:\n",
    "                sentence_graph[i][j] = scoring(sentence1, sentence2, metadata)\n",
    "                sentence_common_graph[i][j] = get_no_of_common_word(sentence1, sentence2)\n",
    "    return sentence_graph, sentence_common_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following functions are different ways to give a score to a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(sentence_graph):\n",
    "    scores = np.zeros(len(sentence_graph))\n",
    "    for i,sentence in enumerate(sentence_graph):\n",
    "        scores[i] = sum(sentence_graph[i])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank_scores(sentence_graph):\n",
    "    N = len(sentence_graph)\n",
    "    d = 0.15   # PageRank Hyperparameter\n",
    "    pagerank_scores = np.ones(N)\n",
    "    \n",
    "    out_degree = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if sentence_graph[i][j]:\n",
    "                out_degree[i] += sentence_graph[i][j]\n",
    "    \n",
    "    for i in range(N):\n",
    "        score = 0\n",
    "        for j in range(N):\n",
    "            if sentence_graph[j][i]:\n",
    "                score += (pagerank_scores[j] / out_degree[j])\n",
    "        pagerank_scores[i] = (d / N) + (1 - d) * score\n",
    "    return pagerank_scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranks the sentences based on any one of the above scoring methods and return the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary2(sentences, processed_sentences, sentence_graph, scores):\n",
    "    scores_indices = np.argsort(scores)\n",
    "    ordered_sentences = scores_indices[::-1]\n",
    "    summary = []\n",
    "    for i in range(5):\n",
    "        summary.append(sentences[ordered_sentences[i]])\n",
    "#         print(ordered_sentences[i], scores[ordered_sentences[i]])\n",
    "#         print(processed_sentences[ordered_sentences[i]])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary(sentences, processed_sentences, sentence_graph, scores, summary_length):\n",
    "    summary = []\n",
    "    for i in range(5): # Number of Sentences we want in the summary\n",
    "        score_indices = np.argsort(scores)\n",
    "        selected_index = score_indices[-1]\n",
    "        summary.append(sentences[selected_index]) # Adding highest score sentence to summary\n",
    "        mean_score = np.mean(sentence_graph)\n",
    "        to_decrease = []\n",
    "        # Calculated mean similarity score. If selected sentence and another sentence have\n",
    "        # high similarity, the score of the second sentence should be reduced.\n",
    "        # Here, have chosen to use 1.5 * mean_score as the threshold, and divided score in half.\n",
    "        for iterator in range(len(processed_sentences)):\n",
    "            if sentence_graph[iterator][selected_index] > 1.5 * mean_score:\n",
    "                to_decrease.append(iterator)\n",
    "            if sentence_graph[selected_index][iterator] > 1.5 * mean_score:\n",
    "                to_decrease.append(iterator)\n",
    "        for sentence in set(to_decrease):\n",
    "            # Should be changed based on the number of sentences needed in the summary\n",
    "            scores[sentence] /= (1 + 1.0 / summary_length) # Reduced score by half, to on average prevent from being picked.\n",
    "        scores[selected_index] = 0\n",
    "    return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Program which calls the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus_path = \"data/arxiv-release/train.txt\"\n",
    "train_corpus_path = \"/media/kaushik/Studies/IIITH/3_ThirdSem/IRE/Major Project/arxiv-release/arxiv-release/train.txt\"\n",
    "#read_data_and_split(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = load_data_from_pickle(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences, sentence_metadata = get_sentences_with_metadata(data_map)\n",
    "list_of_sentences = [sentence.strip() for sentence in list_of_sentences if len(sentence) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = make_processed_sentences(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "sentence_graph, sentence_common_graph = make_graph(processed_sentences, sentence_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = calculate_scores(sentence_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_page_scores = calculate_pagerank_scores(sentence_common_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_score_final = [sentence_scores[i] * (sentence_page_scores[i]+1)  for i in range(len(sentence_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thus , in this section we show that the gradient projection method delivers points in a small neighborhood of the function @xmath288 and , therefore , of the function @xmath289 the size of this neighborhood is proportional to @xmath290 it is convenient to indicate in this section dependencies of the functional @xmath291 from @xmath292 and @xmath293 hence we write in this section @xmath294    * theorem 5',\n",
       " 'we note that in the conventional case of a non convex cost functional a gradient - like method converges to the exact solution only if its starting point is located in a sufficiently small neighborhood of this solution : this is due to the phenomenon of multiple local minima and ravines of such functionals',\n",
       " 'unlike previously developed globally convergent numerical methods of the first type for cips ( see this section below ) , the convergence analysis for the technique of the current paper does not impose a smallness condition on the interval @xmath5 of the variations of the wave numbers @xmath6',\n",
       " '00 ] ) where @xmath72 is replaced with @xmath73 @xmath74      for @xmath75,k\\\\in \\\\lbrack \\\\underline{k},\\\\overline{k}]$ ] consider the function @xmath76 and its @xmath77derivative @xmath78 , where @xmath79    hence,@xmath80consider the function @xmath81 , which we call the tail function \" , and this function is unknown,@xmath82    let @xmath83 note that since @xmath84 for @xmath85 then equation ( [ 2',\n",
       " 'liu , _ numerical solution of a coefficient inverse problem with multi - frequency experimental raw data by a globally convergent algorithm _ , ( 2016 ) , https://arxiv']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_length = 5\n",
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_scores, summary_length)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thus , in this section we show that the gradient projection method delivers points in a small neighborhood of the function @xmath288 and , therefore , of the function @xmath289 the size of this neighborhood is proportional to @xmath290 it is convenient to indicate in this section dependencies of the functional @xmath291 from @xmath292 and @xmath293 hence we write in this section @xmath294    * theorem 5',\n",
       " 'we note that in the conventional case of a non convex cost functional a gradient - like method converges to the exact solution only if its starting point is located in a sufficiently small neighborhood of this solution : this is due to the phenomenon of multiple local minima and ravines of such functionals',\n",
       " 'unlike previously developed globally convergent numerical methods of the first type for cips ( see this section below ) , the convergence analysis for the technique of the current paper does not impose a smallness condition on the interval @xmath5 of the variations of the wave numbers @xmath6',\n",
       " '00 ] ) where @xmath72 is replaced with @xmath73 @xmath74      for @xmath75,k\\\\in \\\\lbrack \\\\underline{k},\\\\overline{k}]$ ] consider the function @xmath76 and its @xmath77derivative @xmath78 , where @xmath79    hence,@xmath80consider the function @xmath81 , which we call the tail function \" , and this function is unknown,@xmath82    let @xmath83 note that since @xmath84 for @xmath85 then equation ( [ 2',\n",
       " 'nguyen , _ a globally convergent numerical method for a 1-d inverse medium problem with experimental data _ , inverse problems and imaging , 10 ( 2016 ) , pp']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_score_final, summary_length)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14 ] ) by @xmath53 , we obtain @xmath54 hence , there exists a function @xmath55 independent on @xmath26 such that @xmath56setting in ( [ 2',\n",
       " '170 ] ) imply that@xmath155@xmath156@xmath157    introduce the hilbert space @xmath158 of pairs of real valued functions @xmath159 @xmath160 as@xmath161 ^{1/2}<\\\\infty% \\\\end{array}% \\\\right\\\\ }',\n",
       " 'the idea of any version of the gcm of the second type has direct roots in the method of @xcite , which is based on carleman estimates and which was originally designed in @xcite only for proofs of uniqueness theorems for cips , also see the recent survey in @xcite',\n",
       " 'unlike previously developed globally convergent numerical methods of the first type for cips ( see this section below ) , the convergence analysis for the technique of the current paper does not impose a smallness condition on the interval @xmath5 of the variations of the wave numbers @xmath6',\n",
       " '00 ] ) where @xmath72 is replaced with @xmath73 @xmath74      for @xmath75,k\\\\in \\\\lbrack \\\\underline{k},\\\\overline{k}]$ ] consider the function @xmath76 and its @xmath77derivative @xmath78 , where @xmath79    hence,@xmath80consider the function @xmath81 , which we call the tail function \" , and this function is unknown,@xmath82    let @xmath83 note that since @xmath84 for @xmath85 then equation ( [ 2']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_page_scores, summary_length)\n",
    "summary\n",
    "# make_processed_sentences(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
