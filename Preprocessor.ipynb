{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the full training data and split it into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_and_split(train_corpus_path):\n",
    "    train_data_map = {}\n",
    "    file_no = 0\n",
    "    with open(train_corpus_path, 'r') as train_data_file:\n",
    "        line_count = 0\n",
    "        while file_no < 11:\n",
    "            if line_count < 20000:\n",
    "                line_data = train_data_file.readline()\n",
    "                if line_data:\n",
    "                    line_map = json.loads(line_data)\n",
    "                    article_id = line_map['article_id']\n",
    "                    del line_map['article_id']\n",
    "                    train_data_map[article_id] = line_map\n",
    "                    line_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "                    pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"File \", file_no, \" Done\")\n",
    "                train_data_map.clear()\n",
    "                file_no += 1\n",
    "                line_count = 0\n",
    "        with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "            pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loads a particular pickle file of the training data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(train_corpus_path):\n",
    "    data_path = train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain\"\n",
    "    data_map = {}\n",
    "    with open(data_path + \"/\" + \"train_0.pickle\", 'rb') as handle:\n",
    "        data_map = pickle.load(handle)\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets all the sentences of the article along with its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_metadata(data_map):\n",
    "    full_text = []\n",
    "    sentence_metadata = []\n",
    "    list_of_sentences = []\n",
    "    c = 0\n",
    "    for article_id, data in data_map.items():\n",
    "        if c == 50:\n",
    "            section_data = data['sections']\n",
    "            section_names = data['section_names']\n",
    "            for i, section in enumerate(section_data):\n",
    "                for line in section:\n",
    "                    split_line = line.split('.')\n",
    "                    for l in split_line:\n",
    "                        list_of_sentences.append(l)\n",
    "                        sentence_metadata.append(section_names[i])\n",
    "        c += 1\n",
    "    return list_of_sentences, sentence_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Following 2 functions are used for Preprocessing of a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(word):\n",
    "    \"\"\"\n",
    "    Checks if word is ascii or not\n",
    "    :param word: token\n",
    "    :return: Boolean\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    try:\n",
    "        word = word.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        valid = False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens(sentence):\n",
    "    punc_map = {}\n",
    "    punc_map = punc_map.fromkeys('!\"\\'()*+,;<>[\\\\]^`{|}~:=%&_#?-$/', ' ')\n",
    "    table = str.maketrans(punc_map)\n",
    "    tokens = sentence.lower().translate(table).split()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and is_ascii(word) and '@' not in word and len(word) > 1]            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets the processed sentences for each sentence of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_sentences(list_of_sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = \" \".join(sentence)\n",
    "        processed_sentences.append(get_processed_tokens(sentence))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gives the number of words common between given 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_of_common_word(sentence1, sentence2):\n",
    "    common_count = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2:\n",
    "            if s1 == s2:\n",
    "                common_count += 1\n",
    "    return common_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic scoring function which gives a score between 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(sentence1, sentence2, metadata):\n",
    "    common_words = get_no_of_common_word(sentence1, sentence2)\n",
    "    score = common_words\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Makes the graph which has relations between every pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(processed_sentences, metadata):\n",
    "    sentence_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    for i in range(len(processed_sentences)):\n",
    "        for j in range(len(processed_sentences)):\n",
    "            sentence1 = processed_sentences[i]\n",
    "            sentence2 = processed_sentences[j]\n",
    "            if i == j:\n",
    "                sentence_graph[i][j] = 0\n",
    "            else:\n",
    "                sentence_graph[i][j] = scoring(sentence1, sentence2, metadata)\n",
    "    return sentence_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following functions are different ways to give a score to a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(sentence_graph):\n",
    "    scores = np.zeros(len(sentence_graph))\n",
    "    for i,sentence in enumerate(sentence_graph):\n",
    "        scores[i] = sum(sentence_graph[i])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank_scores(sentence_graph):\n",
    "    N = len(sentence_graph)\n",
    "    d = 0.15   # PageRank Hyperparameter\n",
    "    pagerank_scores = np.ones(N)\n",
    "    \n",
    "    out_degree = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if sentence_graph[i][j]:\n",
    "                out_degree[i] += sentence_graph[i][j]\n",
    "    \n",
    "    for i in range(N):\n",
    "        score = 0\n",
    "        for j in range(N):\n",
    "            if sentence_graph[j][i]:\n",
    "                score += (pagerank_scores[j] / out_degree[j])\n",
    "        pagerank_scores[i] = (d / N) + (1 - d) * score\n",
    "    return pagerank_scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranks the sentences based on any one of the above scoring methods and return the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary(sentences, processed_sentences, sentence_graph, scores):\n",
    "    scores_indices = np.argsort(scores)\n",
    "    ordered_sentences = scores_indices[::-1]\n",
    "    summary = []\n",
    "    for i in range(5):\n",
    "        summary.append(sentences[ordered_sentences[i]])\n",
    "#         print(ordered_sentences[i], scores[ordered_sentences[i]])\n",
    "#         print(processed_sentences[ordered_sentences[i]])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary(sentences, processed_sentences, sentence_graph, scores):\n",
    "    summary = []\n",
    "    for i in range(5): # Number of Sentences we want in the summary\n",
    "        score_indices = np.argsort(scores)\n",
    "        selected_index = score_indices[-1]\n",
    "        summary.append(sentences[selected_index]) # Adding highest score sentence to summary\n",
    "        mean_score = np.mean(sentence_graph)\n",
    "        to_decrease = []\n",
    "        # Calculated mean similarity score. If selected sentence and another sentence have\n",
    "        # high similarity, the score of the second sentence should be reduced.\n",
    "        # Here, have chosen to use 1.5 * mean_score as the threshold, and divided score in half.\n",
    "        for iterator in range(len(processed_sentences)):\n",
    "            if sentence_graph[iterator][selected_index] > 1.5 * mean_score:\n",
    "                to_decrease.append(iterator)\n",
    "            if sentence_graph[selected_index][iterator] > 1.5 * mean_score:\n",
    "                to_decrease.append(iterator)\n",
    "        for sentence in set(to_decrease):\n",
    "            scores[sentence] /= 2 # Reduced score by half, to on average prevent from being picked.\n",
    "        scores[selected_index] = 0\n",
    "    return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Program which calls the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_path = \"data/arxiv-release/train.txt\"\n",
    "#read_data_and_split(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = load_data_from_pickle(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences, sentence_metadata = get_sentences_with_metadata(data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = make_processed_sentences(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_graph = make_graph(processed_sentences, sentence_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = calculate_scores(sentence_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we see that for @xmath129 there exists an intermediate region between the critical point @xmath130 at which @xmath131 for the * af * phase , characterizing a second - order transition , and the point @xmath132 at which the @xmath133 order parameter presents a discontinuity for the * caf * phase , characterizing a first - order transition ',\n",
       " '\\\\tag{10}%\\\\end{aligned}\\\\ ] ]    to obtain the minimum energy with a boundary condition given by normalization @xmath98 , we use the lagrange multiplier method which correspond the minimization of the functional@xmath99    the stationary solutions ( @xmath100 ) are obtained by solving the set of nonlinear equations @xmath101{c}% -\\\\left (   \\\\lambda+1\\\\right )   x-4\\\\left (   \\\\lambda+1\\\\right )   xu^{2}+2\\\\left ( y\\\\lambda+z\\\\right )   + 12\\\\alpha xu^{2}=2\\\\eta x\\\\\\\\ \\\\left (   1-\\\\lambda\\\\right )   \\\\left (   y+4yv^{2}\\\\right )   + 2x\\\\lambda-2\\\\alpha\\\\left ( y - z\\\\right )   -12\\\\alpha yv^{2}=2\\\\eta y\\\\\\\\ -\\\\left (   1-\\\\lambda\\\\right )   z-4\\\\left (   1-\\\\lambda\\\\right )   z\\\\omega^{2}% + 2x+2\\\\alpha\\\\left (',\n",
       " 'et al__@xcite , by using coupled cluster treatment found the surprising and novel result that there exists a quantum triple point ( * qtp * ) with coordinates at ( @xmath43 ) , below which there is a second - order phase transition between the * af * and * caf * phases while above this * qtp * are these two ordered phases separated by the intermediate magnetically disordered phase ( vbs or rvb ) ',\n",
       " 'the ground state vector @xmath105 is an eigenvector of @xmath106 , where @xmath107 is the total spin of the @xmath70th plaquette of four spins , with zero eigenvalue ( singlet state ) ',\n",
       " 'y - z\\\\right )   -12\\\\alpha z\\\\omega^{2}=2\\\\eta z\\\\\\\\ -\\\\left (   \\\\lambda+1\\\\right )   u=2\\\\eta u\\\\\\\\ \\\\left (   1-\\\\lambda\\\\right )   v-2\\\\alpha v=2\\\\eta v\\\\\\\\ -\\\\left (   1-\\\\lambda\\\\right )   \\\\omega-2\\\\alpha\\\\omega=2\\\\eta\\\\omega \\\\end{array } \\\\right ']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_scores)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = calculate_pagerank_scores(sentence_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' richter , _ phys ',\n",
       " ' therefore , in the limit of the not frustrated ( @xmath136 ) square lattice ( @xmath67 ) antiferromagnetic , solving the equations ( 12 ) and applying the corrections factor we found @xmath146 which is consistent with the numerical results obtained by various methods such as series expansion , quantum monte carlo simulation , and others@xcite , and can also be compared with experimental results for the k@xmath24nif@xmath25 , k@xmath24mnf@xmath25 , and rb@xmath147mnf@xmath25 compounds@xcite ',\n",
       " '* 88 * , 047601 ( 2002 ) ; p',\n",
       " ' papinutto , and p',\n",
       " ' li , r']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_scores)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
