{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing the Word2Vec Model, download the file from https://nlp.stanford.edu/projects/glove/\n",
    "# Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip \n",
    "# Unzip the file then run: python3 -m gensim.scripts.glove2word2vec --input  glove.6B.300d.txt --output glove.6B.300d.w2vformat.txt\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/glove.6B.300d.w2vformat.txt')\n",
    "word_freq_map = {}\n",
    "# with open(\"vocab\", 'r') as vocab_file:\n",
    "with open(\"data/arxiv-release/vocab\", 'r') as vocab_file:\n",
    "    lines = vocab_file.readlines()\n",
    "    for line in lines:\n",
    "        word_freq_map[line.split()[0]] = int(line.split()[1])\n",
    "    \n",
    "stop_list = sorted(word_freq_map.items(), key=operator.itemgetter(1), reverse=True)[:150]\n",
    "cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the full training data and split it into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_and_split(train_corpus_path):\n",
    "    train_data_map = {}\n",
    "    file_no = 0\n",
    "    with open(train_corpus_path, 'r') as train_data_file:\n",
    "        line_count = 0\n",
    "        while file_no < 11:\n",
    "            if line_count < 20000:\n",
    "                line_data = train_data_file.readline()\n",
    "                if line_data:\n",
    "                    line_map = json.loads(line_data)\n",
    "                    article_id = line_map['article_id']\n",
    "                    del line_map['article_id']\n",
    "                    train_data_map[article_id] = line_map\n",
    "                    line_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "                    pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"File \", file_no, \" Done\")\n",
    "                train_data_map.clear()\n",
    "                file_no += 1\n",
    "                line_count = 0\n",
    "        with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "            pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loads a particular pickle file of the training data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(train_corpus_path):\n",
    "    data_path = train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain\"\n",
    "    data_map = {}\n",
    "    with open(data_path + \"/\" + \"train_0.pickle\", 'rb') as handle:\n",
    "        data_map = pickle.load(handle)\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets all the sentences of the article along with its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_metadata(data_map):\n",
    "    full_text = []\n",
    "    sentence_metadata = []\n",
    "    list_of_sentences = []\n",
    "    c = 0\n",
    "    for article_id, data in data_map.items():\n",
    "        if c == 60:\n",
    "            section_data = data['sections']\n",
    "            section_names = data['section_names']\n",
    "            for i, section in enumerate(section_data):\n",
    "                for line in section:\n",
    "                    split_line = line.split('.')\n",
    "                    for l in split_line:\n",
    "                        list_of_sentences.append(l)\n",
    "                        sentence_metadata.append(section_names[i])\n",
    "        c += 1\n",
    "    return list_of_sentences, sentence_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Following 2 functions are used for Preprocessing of a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(word):\n",
    "    \"\"\"\n",
    "    Checks if word is ascii or not\n",
    "    :param word: token\n",
    "    :return: Boolean\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    try:\n",
    "        word = word.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        valid = False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens(sentence):\n",
    "    punc_map = {}\n",
    "    punc_map = punc_map.fromkeys('!\"\\'()*+,;<>[]^`{|}~:=%&_#?-$/', ' ')\n",
    "    table = str.maketrans(punc_map)\n",
    "    tokens = sentence.lower().translate(table).split()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stop_words = list(stop_words) + stop_list\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and is_ascii(word) and '@' not in word and '\\\\' not in word and len(word) > 1]            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets the processed sentences for each sentence of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_sentences(list_of_sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = \" \".join(sentence)\n",
    "        processed_sentences.append(get_processed_tokens(sentence))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gives the number of words common between given 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_of_common_word(sentence1, sentence2):\n",
    "    common_count = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2:\n",
    "            if s1 == s2:\n",
    "                common_count += 1\n",
    "    return common_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vec_sim(sentence1, sentence2):\n",
    "    score = 0\n",
    "    for word1 in sentence1:\n",
    "        for word2 in sentence2:\n",
    "            try:\n",
    "                temp = cache[word1+word2]\n",
    "            except:\n",
    "                try:\n",
    "                    temp = model.similarity(word1, word2)\n",
    "                    cache[word1+word2] = temp\n",
    "                    cache[word2+word1] = temp\n",
    "                except:\n",
    "                    cache[word1+word2] = 0\n",
    "                    cache[word2+word1] = 0\n",
    "                    temp = 0\n",
    "            score += temp\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic scoring function which gives a score between 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(sentence1, sentence2, metadata):\n",
    "    len_normalize = len(sentence1) + len(sentence2) + 1 # Normalizing by length of vector\n",
    "    common_words = get_no_of_common_word(sentence1, sentence2)\n",
    "    word_vec_score = get_word_vec_sim(sentence1, sentence2)\n",
    "    score = common_words / 2*len_normalize + word_vec_score / len_normalize\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Makes the graph which has relations between every pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(processed_sentences, metadata):\n",
    "    sentence_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    sentence_common_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    \n",
    "    for i in range(len(processed_sentences)):\n",
    "        for j in range(len(processed_sentences)):\n",
    "            sentence1 = processed_sentences[i]\n",
    "            sentence2 = processed_sentences[j]\n",
    "            if i == j:\n",
    "                sentence_graph[i][j] = 0\n",
    "                sentence_common_graph[i][j] = 0\n",
    "            else:\n",
    "                sentence_graph[i][j] = scoring(sentence1, sentence2, metadata)\n",
    "                sentence_common_graph[i][j] = get_no_of_common_word(sentence1, sentence2)\n",
    "    return sentence_graph, sentence_common_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following functions are different ways to give a score to a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(sentence_graph):\n",
    "    scores = np.zeros(len(sentence_graph))\n",
    "    for i,sentence in enumerate(sentence_graph):\n",
    "        scores[i] = sum(sentence_graph[i])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank_scores(sentence_graph):\n",
    "    N = len(sentence_graph)\n",
    "    d = 0.15   # PageRank Hyperparameter\n",
    "    pagerank_scores = np.ones(N)\n",
    "    \n",
    "    out_degree = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if sentence_graph[i][j]:\n",
    "                out_degree[i] += sentence_graph[i][j]\n",
    "    \n",
    "    for i in range(N):\n",
    "        score = 0\n",
    "        for j in range(N):\n",
    "            if sentence_graph[j][i]:\n",
    "                score += (pagerank_scores[j] / out_degree[j])\n",
    "        pagerank_scores[i] = (d / N) + (1 - d) * score\n",
    "    return pagerank_scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranks the sentences based on any one of the above scoring methods and return the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary2(sentences, processed_sentences, sentence_graph, scores):\n",
    "    scores_indices = np.argsort(scores)\n",
    "    ordered_sentences = scores_indices[::-1]\n",
    "    summary = []\n",
    "    for i in range(5):\n",
    "        summary.append(sentences[ordered_sentences[i]])\n",
    "#         print(ordered_sentences[i], scores[ordered_sentences[i]])\n",
    "#         print(processed_sentences[ordered_sentences[i]])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary(sentences, processed_sentences, sentence_graph, scores, summary_length):\n",
    "    summary = []\n",
    "    for i in range(5): # Number of Sentences we want in the summary\n",
    "        score_indices = np.argsort(scores)\n",
    "        selected_index = score_indices[-1]\n",
    "        summary.append(sentences[selected_index]) # Adding highest score sentence to summary\n",
    "        mean_score = np.mean(sentence_graph)\n",
    "        to_decrease = []\n",
    "        # Calculated mean similarity score. If selected sentence and another sentence have\n",
    "        # high similarity, the score of the second sentence should be reduced.\n",
    "        # Here, have chosen to use 1.5 * mean_score as the threshold, and divided score in half.\n",
    "        for iterator in range(len(processed_sentences)):\n",
    "            if sentence_graph[iterator][selected_index] > 1.5 * mean_score:\n",
    "                to_decrease.append(iterator)\n",
    "            if sentence_graph[selected_index][iterator] > 1.5 * mean_score:\n",
    "                to_decrease.append(iterator)\n",
    "        for sentence in set(to_decrease):\n",
    "            # Should be changed based on the number of sentences needed in the summary\n",
    "            scores[sentence] /= (1 + 1.0 / summary_length) # Reduced score by half, to on average prevent from being picked.\n",
    "        scores[selected_index] = 0\n",
    "    return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Program which calls the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_path = \"data/arxiv-release/train.txt\"\n",
    "# train_corpus_path = \"/media/kaushik/Studies/IIITH/3_ThirdSem/IRE/Major Project/arxiv-release/arxiv-release/train.txt\"\n",
    "#read_data_and_split(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = load_data_from_pickle(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences, sentence_metadata = get_sentences_with_metadata(data_map)\n",
    "list_of_sentences = [sentence.strip() for sentence in list_of_sentences if len(sentence) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = make_processed_sentences(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "sentence_graph, sentence_common_graph = make_graph(processed_sentences, sentence_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = calculate_scores(sentence_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_page_scores = calculate_pagerank_scores(sentence_common_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_score_final = [sentence_scores[i] * (sentence_page_scores[i]+1)  for i in range(len(sentence_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the hamiltonian modeling this system consists of the ground and one excited state of the molecule and a quasi - continuum describing the conduction band together with one vibrational coordinate @xmath15 here @xmath16 can be equal to @xmath17 for the ground state , @xmath18 for the excited state , and @xmath19 for the quasi - continuum',\n",
       " 'first , the vibrational populations in the excited state of the molecule no longer only decay into the quasi - continuum states but also relax within the excited state ( see fig',\n",
       " 'therefore the total hamiltonian consists of three terms  the system part @xmath1 , the bath part @xmath2 , and the system - bath interaction @xmath3 : @xmath4 the rdm @xmath5 is obtained from the density matrix of the full system by tracing out the degrees of freedom of the environment',\n",
       " 'the electronic probabilities in the quasi - continuum are given as @xmath30 where @xmath31 is the initial vibronic distribution in the excited state and @xmath32 and @xmath33 are the vibronic parts of the wave packet in the excited and quasi - continuum states , respectively',\n",
       " 'recently @xcite the electron injection from a chromophore to a semiconductor conduction band was described using the time - dependent schrdinger equation , thus neglecting relaxation processes']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_length = 5\n",
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_scores, summary_length)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the hamiltonian modeling this system consists of the ground and one excited state of the molecule and a quasi - continuum describing the conduction band together with one vibrational coordinate @xmath15 here @xmath16 can be equal to @xmath17 for the ground state , @xmath18 for the excited state , and @xmath19 for the quasi - continuum',\n",
       " 'therefore the total hamiltonian consists of three terms  the system part @xmath1 , the bath part @xmath2 , and the system - bath interaction @xmath3 : @xmath4 the rdm @xmath5 is obtained from the density matrix of the full system by tracing out the degrees of freedom of the environment',\n",
       " 'first , the vibrational populations in the excited state of the molecule no longer only decay into the quasi - continuum states but also relax within the excited state ( see fig',\n",
       " 'recently @xcite the electron injection from a chromophore to a semiconductor conduction band was described using the time - dependent schrdinger equation , thus neglecting relaxation processes',\n",
       " 'the electronic probabilities in the quasi - continuum are given as @xmath30 where @xmath31 is the initial vibronic distribution in the excited state and @xmath32 and @xmath33 are the vibronic parts of the wave packet in the excited and quasi - continuum states , respectively']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_score_final, summary_length)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if one assumes bilinear system - bath coupling with system part @xmath8 and bath part @xmath9 @xmath10 one can take advantage of the following decomposition @xcite : @xmath11   +    [ \\\\lambda\\\\rho , k]+   [ k,\\\\rho\\\\lambda^{\\\\dagger } ]',\n",
       " 'recently @xcite the electron injection from a chromophore to a semiconductor conduction band was described using the time - dependent schrdinger equation , thus neglecting relaxation processes',\n",
       " 'we are aware that this is only a minimal model but hope that it catches the effects of dissipation on the electron injection process',\n",
       " 'the hamiltonian modeling this system consists of the ground and one excited state of the molecule and a quasi - continuum describing the conduction band together with one vibrational coordinate @xmath15 here @xmath16 can be equal to @xmath17 for the ground state , @xmath18 for the excited state , and @xmath19 for the quasi - continuum',\n",
       " 'to be able to study the effects of dissipation']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_page_scores, summary_length)\n",
    "summary\n",
    "# make_processed_sentences(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
