{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_and_split(train_corpus_path):\n",
    "    \"\"\"\n",
    "    Read the full training data and split it into smaller chunks\n",
    "    :param train_corpus_path: input path\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_data_map = {}\n",
    "    file_no = 0\n",
    "    with open(train_corpus_path, 'r') as train_data_file:\n",
    "        line_count = 0\n",
    "        while file_no < 11:\n",
    "            if line_count < 20000:\n",
    "                line_data = train_data_file.readline()\n",
    "                if line_data:\n",
    "                    line_map = json.loads(line_data)\n",
    "                    article_id = line_map['article_id']\n",
    "                    del line_map['article_id']\n",
    "                    train_data_map[article_id] = line_map\n",
    "                    line_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "                    pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"File \", file_no, \" Done\")\n",
    "                train_data_map.clear()\n",
    "                file_no += 1\n",
    "                line_count = 0\n",
    "        with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "            pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_path = \"/media/kaushik/Studies/IIITH/3_ThirdSem/IRE/Major Project/arxiv-release/arxiv-release/train.txt\"\n",
    "# read_data_and_split(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain\"\n",
    "data_map = {}\n",
    "with open(data_path + \"/\" + \"train_0.pickle\", 'rb') as handle:\n",
    "    data_map = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = []\n",
    "for article_id, data in data_map.items():\n",
    "    for part, article_data in data.items():\n",
    "        if part != 'labels' and part != 'section_names':\n",
    "            full_text.append(article_data)\n",
    "    break\n",
    "list_of_sentences = [word for line in full_text for word in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(word):\n",
    "    \"\"\"\n",
    "    Checks if word is ascii or not\n",
    "    :param word: token\n",
    "    :return: Boolean\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    try:\n",
    "        word = word.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        valid = False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens(sentence):\n",
    "    punc_map = {}\n",
    "    punc_map = punc_map.fromkeys('!\"\\'()*+,;<>[\\\\]^`{|}~:=%&_#?-$/', ' ')\n",
    "    table = str.maketrans(punc_map)\n",
    "    tokens = sentence.lower().translate(table).split()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and is_ascii(word) and '@' not in word and len(word) > 1]            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_sentences(list_of_sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = \" \".join(sentence)\n",
    "        processed_sentences.append(get_processed_tokens(sentence))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_of_common_word(sentence1, sentence2):\n",
    "    common_count = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2:\n",
    "            if s1 == s2:\n",
    "                common_count += 1\n",
    "    return common_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(processed_sentences):\n",
    "    sentence_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    for i in range(len(processed_sentences)):\n",
    "        for j in range(len(processed_sentences)):\n",
    "            sentence1 = processed_sentences[i]\n",
    "            sentence2 = processed_sentences[j]\n",
    "            if i == j:\n",
    "                sentence_graph[i][j] = 0\n",
    "            else:\n",
    "                sentence_graph[i][j] = get_no_of_common_word(sentence1, sentence2)\n",
    "    return sentence_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26. 21. 18. 33.]\n"
     ]
    }
   ],
   "source": [
    "def calculate_scores(sentence_graph):\n",
    "    scores = np.zeros(len(sentence_graph))\n",
    "    for i,sentence in enumerate(sentence_graph):\n",
    "        scores[i] = sum(sentence_graph[i])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary(processed_sentences, sentence_graph, scores):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.120e+02 2.460e+02 3.560e+02 1.095e+03 6.460e+02 5.040e+02 1.146e+03\n",
      " 8.220e+02 2.000e+02 5.000e+01 2.000e+01 2.800e+02 1.120e+02 2.520e+02\n",
      " 2.400e+02 2.190e+02 1.000e+02 5.360e+02 2.150e+02 1.500e+02 3.300e+02\n",
      " 1.540e+02 3.900e+01 1.380e+02 1.880e+02 3.200e+01 6.720e+02 8.500e+01\n",
      " 5.400e+01 7.650e+02 5.500e+01 2.120e+02 1.440e+02 3.900e+01 4.630e+02\n",
      " 1.030e+02 3.500e+01 5.010e+02 2.300e+01 1.500e+02 4.700e+01 4.010e+02\n",
      " 8.600e+01 1.700e+02 1.360e+02 5.290e+02 3.420e+02 1.050e+02 5.600e+01\n",
      " 1.060e+02 9.200e+01 1.220e+02 1.750e+02 2.750e+02 2.330e+02 2.480e+02\n",
      " 2.430e+02 6.100e+01 9.100e+01 1.760e+02 2.800e+01 1.011e+03 2.800e+02\n",
      " 3.200e+02 1.100e+02 3.370e+02 2.970e+02 1.730e+02 1.070e+02 8.860e+02\n",
      " 5.400e+01 3.200e+01 2.100e+02 2.900e+01 4.040e+02 1.750e+02 1.170e+02\n",
      " 1.520e+02 3.020e+02 1.340e+02 9.140e+02 1.920e+02 1.410e+02 1.870e+02\n",
      " 8.700e+01 1.350e+02 1.430e+02 1.680e+02 5.200e+02 8.800e+02 2.770e+02\n",
      " 5.660e+02 4.900e+01 4.360e+02 3.600e+01 2.700e+02 2.330e+02 9.900e+01\n",
      " 1.270e+02 6.990e+02 1.330e+02 1.680e+02 3.840e+02 5.740e+02 2.480e+02\n",
      " 5.070e+02 3.300e+01 2.000e+01 1.710e+02 1.700e+02 6.000e+00 1.900e+01\n",
      " 1.150e+02 4.700e+01 0.000e+00 6.690e+02 1.710e+02 4.400e+01 2.300e+01\n",
      " 1.110e+02 3.500e+01 8.200e+01 7.080e+02 2.330e+02 1.300e+01 1.190e+02\n",
      " 3.000e+00 1.000e+00 1.300e+02 3.940e+02 8.410e+02 7.640e+02 3.370e+02\n",
      " 9.386e+03 8.509e+03 7.844e+03]\n"
     ]
    }
   ],
   "source": [
    "processed_sentences = make_processed_sentences(list_of_sentences)\n",
    "sentence_graph = make_graph(processed_sentences)\n",
    "sentence_scores = calculate_scores_and_rank(sentence_graph)\n",
    "summary = rank_sentences_and_make_summary(processed_sentences, sentence_graph, sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
