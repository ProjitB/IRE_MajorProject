{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing the Word2Vec Model, download the file from https://nlp.stanford.edu/projects/glove/\n",
    "# Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip \n",
    "# Unzip the file then run: python3 -m gensim.scripts.glove2word2vec --input  glove.6B.300d.txt --output glove.6B.300d.w2vformat.txt\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/glove.6B.300d.w2vformat.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_map = {}\n",
    "with open(\"data/arxiv-release/vocab\", 'r') as vocab_file:\n",
    "    lines = vocab_file.readlines()\n",
    "    for line in lines:\n",
    "        word_freq_map[line.split()[0]] = int(line.split()[1])\n",
    "    \n",
    "stop_list = sorted(word_freq_map.items(), key=operator.itemgetter(1), reverse=True)[:150]\n",
    "cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the full training data and split it into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_and_split(train_corpus_path):\n",
    "    train_data_map = {}\n",
    "    file_no = 0\n",
    "    with open(train_corpus_path, 'r') as train_data_file:\n",
    "        line_count = 0\n",
    "        while file_no < 11:\n",
    "            if line_count < 20000:\n",
    "                line_data = train_data_file.readline()\n",
    "                if line_data:\n",
    "                    line_map = json.loads(line_data)\n",
    "                    article_id = line_map['article_id']\n",
    "                    del line_map['article_id']\n",
    "                    train_data_map[article_id] = line_map\n",
    "                    line_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "                    pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"File \", file_no, \" Done\")\n",
    "                train_data_map.clear()\n",
    "                file_no += 1\n",
    "                line_count = 0\n",
    "        with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "            pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loads a particular pickle file of the training data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(train_corpus_path):\n",
    "    data_path = train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain\"\n",
    "    data_map = {}\n",
    "    with open(data_path + \"/\" + \"train_0.pickle\", 'rb') as handle:\n",
    "        data_map = pickle.load(handle)\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets all the sentences of the article along with its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_metadata(data_map):\n",
    "    full_text = []\n",
    "    sentence_metadata = []\n",
    "    list_of_sentences = []\n",
    "    summary_list = []\n",
    "    abstract_list = []\n",
    "    c = 0\n",
    "    file_number = 1\n",
    "    for article_id, data in tqdm(data_map.items()):\n",
    "        if c < 10:\n",
    "            abstract_list.append(data['abstract_text'])\n",
    "            section_data = data['sections']\n",
    "            section_names = data['section_names']\n",
    "            for i, section in enumerate(section_data):\n",
    "                for line in section:\n",
    "                    split_line = line.split('.')\n",
    "                    for l in split_line:\n",
    "                        list_of_sentences.append(l)\n",
    "                        sentence_metadata.append(section_names[i])\n",
    "            summary_list.append(do_stuff_and_get_summary(list_of_sentences, sentence_metadata))\n",
    "            list_of_sentences.clear()\n",
    "        else:\n",
    "            c = 0\n",
    "            write_summary_and_abstract_to_file(summary_list, abstract_list, file_number)\n",
    "            print(\"File \", file_number, \" done\")\n",
    "            file_number += 1\n",
    "            summary_list.clear()\n",
    "            abstract_list.clear()\n",
    "            break\n",
    "        c += 1\n",
    "        \n",
    "    return list_of_sentences, sentence_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Following 2 functions are used for Preprocessing of a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(word):\n",
    "    \"\"\"\n",
    "    Checks if word is ascii or not\n",
    "    :param word: token\n",
    "    :return: Boolean\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    try:\n",
    "        word = word.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        valid = False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens(sentence):\n",
    "    punc_map = {}\n",
    "    punc_map = punc_map.fromkeys('!\"\\'()*+,;<>[]^`{|}~:=%&_#?-$/', ' ')\n",
    "    table = str.maketrans(punc_map)\n",
    "    tokens = sentence.lower().translate(table).split()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stop_words = list(stop_words) + stop_list\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and is_ascii(word) and '@' not in word and '\\\\' not in word and len(word) > 1]            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets the processed sentences for each sentence of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_sentences(list_of_sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = \" \".join(sentence)\n",
    "        processed_sentences.append(get_processed_tokens(sentence))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gives the number of words common between given 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_of_common_word(sentence1, sentence2):\n",
    "    common_count = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2:\n",
    "            if s1 == s2:\n",
    "                common_count += 1\n",
    "    return common_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vec_sim(sentence1, sentence2):\n",
    "    score = 0\n",
    "    for word1 in sentence1:\n",
    "        for word2 in sentence2:\n",
    "            try:\n",
    "                temp = cache[word1+word2]\n",
    "            except:\n",
    "                try:\n",
    "                    temp = model.similarity(word1, word2)\n",
    "                    cache[word1+word2] = temp\n",
    "                    cache[word2+word1] = temp\n",
    "                except:\n",
    "                    cache[word1+word2] = 0\n",
    "                    cache[word2+word1] = 0\n",
    "                    temp = 0\n",
    "            score += temp\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic scoring function which gives a score between 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(sentence1, sentence2, metadata):\n",
    "    len_normalize = len(sentence1) + len(sentence2) + 1 # Normalizing by length of vector\n",
    "    common_words = get_no_of_common_word(sentence1, sentence2)\n",
    "    word_vec_score = get_word_vec_sim(sentence1, sentence2)\n",
    "    score = common_words / 2*len_normalize + word_vec_score / len_normalize\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Makes the graph which has relations between every pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(processed_sentences, metadata):\n",
    "    sentence_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    sentence_common_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    \n",
    "    for i in range(len(processed_sentences)):\n",
    "        for j in range(len(processed_sentences)):\n",
    "            sentence1 = processed_sentences[i]\n",
    "            sentence2 = processed_sentences[j]\n",
    "            if i == j:\n",
    "                sentence_graph[i][j] = 0\n",
    "                sentence_common_graph[i][j] = 0\n",
    "            else:\n",
    "                sentence_graph[i][j] = scoring(sentence1, sentence2, metadata)\n",
    "                sentence_common_graph[i][j] = get_no_of_common_word(sentence1, sentence2)\n",
    "    return sentence_graph, sentence_common_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following functions are different ways to give a score to a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(sentence_graph):\n",
    "    scores = np.zeros(len(sentence_graph))\n",
    "    for i,sentence in enumerate(sentence_graph):\n",
    "        scores[i] = sum(sentence_graph[i])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank_scores(sentence_graph):\n",
    "    N = len(sentence_graph)\n",
    "    d = 0.15   # PageRank Hyperparameter\n",
    "    pagerank_scores = np.ones(N)\n",
    "    \n",
    "    out_degree = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if sentence_graph[i][j]:\n",
    "                out_degree[i] += sentence_graph[i][j]\n",
    "    \n",
    "    for i in range(N):\n",
    "        score = 0\n",
    "        for j in range(N):\n",
    "            if sentence_graph[j][i]:\n",
    "                score += (pagerank_scores[j] / out_degree[j])\n",
    "        pagerank_scores[i] = (d / N) + (1 - d) * score\n",
    "    return pagerank_scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranks the sentences based on any one of the above scoring methods and return the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary2(sentences, processed_sentences, sentence_graph, scores):\n",
    "    scores_indices = np.argsort(scores)\n",
    "    ordered_sentences = scores_indices[::-1]\n",
    "    summary = []\n",
    "    for i in range(5):\n",
    "        summary.append(sentences[ordered_sentences[i]])\n",
    "#         print(ordered_sentences[i], scores[ordered_sentences[i]])\n",
    "#         print(processed_sentences[ordered_sentences[i]])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary(sentences, processed_sentences, sentence_graph, scores, summary_length):\n",
    "    summary = []\n",
    "    for i in range(summary_length): # Number of Sentences we want in the summary\n",
    "        score_indices = np.argsort(scores)\n",
    "        if len(score_indices < 1):\n",
    "            break\n",
    "        selected_index = score_indices[-1]\n",
    "        summary.append(sentences[selected_index]) # Adding highest score sentence to summary\n",
    "        mean_score = np.mean(sentence_graph)\n",
    "        to_decrease = []\n",
    "        # Calculated mean similarity score. If selected sentence and another sentence have\n",
    "        # high similarity, the score of the second sentence should be reduced.\n",
    "        # Here, have chosen to use 1.5 * mean_score as the threshold, and divided score in half.\n",
    "        for iterator in range(len(processed_sentences)):\n",
    "            if sentence_graph[iterator][selected_index] > 1.5 * mean_score:\n",
    "                to_decrease.append(iterator)\n",
    "            if sentence_graph[selected_index][iterator] > 1.5 * mean_score:\n",
    "                to_decrease.append(iterator)\n",
    "        for sentence in set(to_decrease):\n",
    "            # Should be changed based on the number of sentences needed in the summary\n",
    "            scores[sentence] /= (1 + 1.0 / summary_length) # Reduced score by half, to on average prevent from being picked.\n",
    "        scores[selected_index] = 0\n",
    "    return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Program which calls the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_path = \"data/arxiv-release/train.txt\"\n",
    "# train_corpus_path = \"/media/kaushik/Studies/IIITH/3_ThirdSem/IRE/Major Project/arxiv-release/arxiv-release/train.txt\"\n",
    "# read_data_and_split(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = load_data_from_pickle(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_stuff_and_get_summary(list_of_sentences, sentence_metadata):\n",
    "    list_of_sentences = [sentence.strip() for sentence in list_of_sentences if len(sentence) > 1]\n",
    "    processed_sentences = make_processed_sentences(list_of_sentences)\n",
    "    sentence_graph, sentence_common_graph = make_graph(processed_sentences, sentence_metadata)\n",
    "    sentence_scores = calculate_scores(sentence_graph)\n",
    "    sentence_page_scores = calculate_pagerank_scores(sentence_common_graph)\n",
    "    sentence_score_final = [sentence_scores[i] * (sentence_page_scores[i]+1)for i in range(len(sentence_scores))]\n",
    "    summary_length = 10\n",
    "    summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_score_final, summary_length)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_and_abstract_to_file(summary_list, abstract_list, file_number):\n",
    "    map_of_abstract_summary = {}\n",
    "    \n",
    "    abstract_map = {}\n",
    "    for i, abstract in enumerate(abstract_list):\n",
    "        abstract_map[i] = abstract\n",
    "    \n",
    "    summary_map = {}\n",
    "    for i, summary in enumerate(summary_list):\n",
    "        summary_map[i] = summary\n",
    "    \n",
    "    with open(\"data/map/abstract_file_\" + str(file_number) + \".pickle\", 'wb') as abs_file:\n",
    "        pickle.dump(abstract_map, abs_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(\"data/map/summary_file_\" + str(file_number) + \".pickle\", 'wb') as sum_file:\n",
    "        pickle.dump(summary_map, sum_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call this function and this function is now changed to generate summaries for all articles and store them to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/20000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 1/20000 [00:02<14:17:21,  2.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/20000 [00:04<12:34:21,  2.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 3/20000 [00:07<14:42:03,  2.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/20000 [00:09<13:08:51,  2.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 5/20000 [00:17<22:47:52,  4.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 6/20000 [00:22<23:25:12,  4.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 7/20000 [00:23<19:36:12,  3.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 8/20000 [00:26<17:10:33,  3.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 9/20000 [00:26<13:10:22,  2.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 10/20000 [00:35<23:29:35,  4.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  1  done\n"
     ]
    }
   ],
   "source": [
    "list_of_sentences, sentence_metadata = get_sentences_with_metadata(data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/20000 [00:01<9:28:52,  1.71s/it]\u001b[A\n",
      "  0%|          | 2/20000 [00:02<8:35:29,  1.55s/it]\u001b[A\n",
      "  0%|          | 3/20000 [00:05<11:10:21,  2.01s/it]\u001b[A\n",
      "  0%|          | 4/20000 [00:07<10:16:29,  1.85s/it]\u001b[A\n",
      "  0%|          | 5/20000 [00:14<19:29:25,  3.51s/it]\u001b[A\n",
      "  0%|          | 6/20000 [00:18<20:31:07,  3.69s/it]\u001b[A\n",
      "  0%|          | 7/20000 [00:20<17:16:36,  3.11s/it]\u001b[A\n",
      "  0%|          | 8/20000 [00:22<14:54:40,  2.69s/it]\u001b[A\n",
      "  0%|          | 9/20000 [00:22<11:23:55,  2.05s/it]\u001b[A\n",
      "  0%|          | 10/20000 [00:30<20:26:34,  3.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  1  done\n"
     ]
    }
   ],
   "source": [
    "list_of_sentences, sentence_metadata = get_sentences_with_metadata(data_map)\n",
    "list_of_sentences = [sentence.strip() for sentence in list_of_sentences if len(sentence) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences = make_processed_sentences(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_graph, sentence_common_graph = make_graph(processed_sentences, sentence_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = calculate_scores(sentence_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_page_scores = calculate_pagerank_scores(sentence_common_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_score_final = [sentence_scores[i] * (sentence_page_scores[i]+1)  for i in range(len(sentence_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0e3da818fff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msummary_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_sentences_and_make_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4d446c8e899b>\u001b[0m in \u001b[0;36mrank_sentences_and_make_summary\u001b[0;34m(sentences, processed_sentences, sentence_graph, scores, summary_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Number of Sentences we want in the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mscore_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mselected_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Adding highest score sentence to summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmean_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "summary_length = 10\n",
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_scores, summary_length)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-54ac93019791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_sentences_and_make_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_score_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4d446c8e899b>\u001b[0m in \u001b[0;36mrank_sentences_and_make_summary\u001b[0;34m(sentences, processed_sentences, sentence_graph, scores, summary_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Number of Sentences we want in the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mscore_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mselected_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Adding highest score sentence to summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmean_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_score_final, summary_length)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_page_scores, summary_length)\n",
    "summary\n",
    "# make_processed_sentences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading function\n",
    "data_path = './data/map'\n",
    "with open(data_path + \"/\" + \"summary_file_1.pickle\", 'rb') as handle:\n",
    "    summary_map = pickle.load(handle)\n",
    "len(summary_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
