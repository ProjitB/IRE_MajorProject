{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the full training data and split it into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_and_split(train_corpus_path):\n",
    "    train_data_map = {}\n",
    "    file_no = 0\n",
    "    with open(train_corpus_path, 'r') as train_data_file:\n",
    "        line_count = 0\n",
    "        while file_no < 11:\n",
    "            if line_count < 20000:\n",
    "                line_data = train_data_file.readline()\n",
    "                if line_data:\n",
    "                    line_map = json.loads(line_data)\n",
    "                    article_id = line_map['article_id']\n",
    "                    del line_map['article_id']\n",
    "                    train_data_map[article_id] = line_map\n",
    "                    line_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "                    pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"File \", file_no, \" Done\")\n",
    "                train_data_map.clear()\n",
    "                file_no += 1\n",
    "                line_count = 0\n",
    "        with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "            pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loads a particular pickle file of the training data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(train_corpus_path):\n",
    "    data_path = train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain\"\n",
    "    data_map = {}\n",
    "    with open(data_path + \"/\" + \"train_0.pickle\", 'rb') as handle:\n",
    "        data_map = pickle.load(handle)\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets all the sentences of the article along with its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_metadata(data_map):\n",
    "    full_text = []\n",
    "    sentence_metadata = []\n",
    "    list_of_sentences = []\n",
    "    c = 0\n",
    "    for article_id, data in data_map.items():\n",
    "        if c == 50:\n",
    "            section_data = data['sections']\n",
    "            section_names = data['section_names']\n",
    "            for i, section in enumerate(section_data):\n",
    "                for line in section:\n",
    "                    split_line = line.split('.')\n",
    "                    for l in split_line:\n",
    "                        list_of_sentences.append(l)\n",
    "                        sentence_metadata.append(section_names[i])\n",
    "        c += 1\n",
    "    return list_of_sentences, sentence_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Following 2 functions are used for Preprocessing of a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(word):\n",
    "    \"\"\"\n",
    "    Checks if word is ascii or not\n",
    "    :param word: token\n",
    "    :return: Boolean\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    try:\n",
    "        word = word.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        valid = False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens(sentence):\n",
    "    punc_map = {}\n",
    "    punc_map = punc_map.fromkeys('!\"\\'()*+,;<>[\\\\]^`{|}~:=%&_#?-$/', ' ')\n",
    "    table = str.maketrans(punc_map)\n",
    "    tokens = sentence.lower().translate(table).split()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and is_ascii(word) and '@' not in word and len(word) > 1]            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets the processed sentences for each sentence of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_sentences(list_of_sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = \" \".join(sentence)\n",
    "        processed_sentences.append(get_processed_tokens(sentence))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gives the number of words common between given 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_of_common_word(sentence1, sentence2):\n",
    "    common_count = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2:\n",
    "            if s1 == s2:\n",
    "                common_count += 1\n",
    "    return common_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic scoring function which gives a score between 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(sentence1, sentence2, metadata):\n",
    "    common_words = get_no_of_common_word(sentence1, sentence2)\n",
    "    score = common_words\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Makes the graph which has relations between every pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(processed_sentences, metadata):\n",
    "    sentence_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    for i in range(len(processed_sentences)):\n",
    "        for j in range(len(processed_sentences)):\n",
    "            sentence1 = processed_sentences[i]\n",
    "            sentence2 = processed_sentences[j]\n",
    "            if i == j:\n",
    "                sentence_graph[i][j] = 0\n",
    "            else:\n",
    "                sentence_graph[i][j] = scoring(sentence1, sentence2, metadata)\n",
    "    return sentence_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following functions are different ways to give a score to a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(sentence_graph):\n",
    "    scores = np.zeros(len(sentence_graph))\n",
    "    for i,sentence in enumerate(sentence_graph):\n",
    "        scores[i] = sum(sentence_graph[i])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank_scores(sentence_graph):\n",
    "    N = len(sentence_graph)\n",
    "    d = 0.15   # PageRank Hyperparameter\n",
    "    pagerank_scores = np.ones(N)\n",
    "    \n",
    "    out_degree = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if sentence_graph[i][j]:\n",
    "                out_degree[i] += sentence_graph[i][j]\n",
    "    \n",
    "    for i in range(N):\n",
    "        score = 0\n",
    "        for j in range(N):\n",
    "            if sentence_graph[j][i]:\n",
    "                score += (pagerank_scores[j] / out_degree[j])\n",
    "        pagerank_scores[i] = (d / N) + (1 - d) * score\n",
    "    return pagerank_scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranks the sentences based on any one of the above scoring methods and return the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary(sentences, processed_sentences, sentence_graph, scores):\n",
    "    scores_indices = np.argsort(scores)\n",
    "    ordered_sentences = scores_indices[::-1]\n",
    "    summary = []\n",
    "    for i in range(5):\n",
    "        summary.append(sentences[ordered_sentences[i]])\n",
    "#         print(ordered_sentences[i], scores[ordered_sentences[i]])\n",
    "#         print(processed_sentences[ordered_sentences[i]])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Program which calls the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_path = \"/media/kaushik/Studies/IIITH/3_ThirdSem/IRE/Major Project/arxiv-release/arxiv-release/train.txt\"\n",
    "#read_data_and_split(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = load_data_from_pickle(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences, sentence_metadata = get_sentences_with_metadata(data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = make_processed_sentences(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_graph = make_graph(processed_sentences, sentence_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = calculate_scores(sentence_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we see that for @xmath129 there exists an intermediate region between the critical point @xmath130 at which @xmath131 for the * af * phase , characterizing a second - order transition , and the point @xmath132 at which the @xmath133 order parameter presents a discontinuity for the * caf * phase , characterizing a first - order transition ',\n",
       " 'et al__@xcite , by using coupled cluster treatment found the surprising and novel result that there exists a quantum triple point ( * qtp * ) with coordinates at ( @xmath43 ) , below which there is a second - order phase transition between the * af * and * caf * phases while above this * qtp * are these two ordered phases separated by the intermediate magnetically disordered phase ( vbs or rvb ) ',\n",
       " 'we have observed , by analyzing the order parameters of the * af * and * caf * phases , that the phase transitions are of second and first - order between the * af - qp * and * caf - qp * , respectively ',\n",
       " 'the frustration contributes significantly to the existence of a disordered intermediate state ( * qp * ) between the two * af * and * caf * ordered phases , while for @xmath151 , we have a direct first - order transition between the * af * and * caf * phases ',\n",
       " ', the line splits into two phase transitions , where the ordered states ( * af * and * caf * ) are separated by an intermediate quantum paramagnetic ( * qp * ) phase , both on a square lattice ']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_scores)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.50041675e-01 4.96688742e-04 6.23719663e-01 4.96688742e-04\n",
      " 9.96786503e-01 4.96688742e-04 1.25145334e+00 4.96688742e-04\n",
      " 1.32921149e+00 8.43446648e-01 4.96688742e-04 2.86605820e-01\n",
      " 4.96688742e-04 7.94181331e-01 1.22868249e-01 4.96688742e-04\n",
      " 7.11333475e-01 4.96688742e-04 1.50210974e-01 8.62790609e-01\n",
      " 4.96688742e-04 6.37235565e-01 4.96688742e-04 4.88506231e-01\n",
      " 4.96688742e-04 1.05118405e-01 5.83354531e-01 4.96688742e-04\n",
      " 5.21756948e-01 2.85519537e-01 3.16063686e-01 4.96688742e-04\n",
      " 4.96688742e-04 5.38624408e-02 4.08360140e-01 4.96688742e-04\n",
      " 1.39058776e-01 6.04603047e-02 4.46673305e-01 4.96688742e-04\n",
      " 6.79820418e-01 4.96688742e-04 3.50772298e-01 4.96688742e-04\n",
      " 3.96716551e-01 1.99973208e-01 3.75375581e-01 4.96688742e-04\n",
      " 3.08033432e-01 4.96688742e-04 3.19192524e-01 4.96688742e-04\n",
      " 2.66493943e-01 4.96688742e-04 1.10119312e+00 4.96688742e-04\n",
      " 5.31617901e-01 1.28955312e+00 4.96688742e-04 5.89912943e-01\n",
      " 4.96688742e-04 2.77980302e-01 4.96688742e-04 8.67117240e-01\n",
      " 4.96688742e-04 1.34197778e+00 4.96688742e-04 4.96688742e-04\n",
      " 3.80055839e-01 4.96688742e-04 3.32095367e-01 4.96688742e-04\n",
      " 1.05221275e-01 4.96688742e-04 7.94349775e-02 4.96688742e-04\n",
      " 1.35685019e-01 1.03843342e-01 4.96688742e-04 2.72100491e-01\n",
      " 5.38229997e-01 1.38114145e-01 4.96688742e-04 2.77093310e-01\n",
      " 4.96688742e-04 3.59706961e-01 4.96688742e-04 2.66593118e-01\n",
      " 4.96688742e-04 7.79459581e-01 2.39307165e-01 6.59601191e-01\n",
      " 3.71060471e-02 3.79168576e-01 4.96688742e-04 6.20288355e-01\n",
      " 4.96688742e-04 7.53635133e-01 5.12703389e-01 4.96688742e-04\n",
      " 2.06702341e-01 4.96688742e-04 7.31436424e-01 4.96688742e-04\n",
      " 1.88369532e-02 3.60368576e-01 4.96688742e-04 2.56908157e-02\n",
      " 4.96688742e-04 6.61572413e-01 4.96688742e-04 1.42711967e-01\n",
      " 4.96688742e-04 2.01405574e-01 4.96688742e-04 6.14850699e-01\n",
      " 4.63215786e-01 4.96688742e-04 2.56990986e-01 2.36702463e-01\n",
      " 7.02634073e-01 4.96688742e-04 4.39617161e-01 4.96688742e-04\n",
      " 2.83802734e-01 4.96688742e-04 9.02630054e-01 3.24514821e-02\n",
      " 3.04128545e-01 4.96688742e-04 3.15215010e-01 4.96688742e-04\n",
      " 1.87657819e-01 1.70292688e-01 4.27309027e-01 4.96688742e-04\n",
      " 3.22902353e-01 5.72977932e-02 4.96688742e-04 4.14299271e-01\n",
      " 1.33234801e-01 4.96688742e-04 9.66748566e-01 4.96688742e-04\n",
      " 4.75124539e-01 4.96688742e-04 1.61023739e-01 4.96688742e-04\n",
      " 2.88522218e-01 4.96688742e-04 4.96688742e-04 4.96688742e-04\n",
      " 3.64754267e-01 1.17719572e-01 2.40388441e-01 4.96688742e-04\n",
      " 1.72825597e-01 4.96688742e-04 6.68016599e-02 4.96688742e-04\n",
      " 4.96688742e-04 3.40154896e-02 6.94929160e-01 4.96688742e-04\n",
      " 1.91282900e-01 4.96688742e-04 4.96688742e-04 8.30982005e-03\n",
      " 4.96688742e-04 3.42291971e-01 1.18043829e-01 2.41150189e-02\n",
      " 4.96688742e-04 2.60070555e-01 2.13176011e-01 1.73483623e-01\n",
      " 4.96688742e-04 1.63481256e-01 1.75561044e-01 2.55644675e-01\n",
      " 7.38600574e-02 2.10456290e-01 2.86615772e-01 4.96688742e-04\n",
      " 3.15204251e-01 4.96688742e-04 2.72429101e-01 3.46498256e-01\n",
      " 4.96688742e-04 3.78053508e-01 4.96688742e-04 5.03653568e-01\n",
      " 4.96688742e-04 2.42612073e-01 4.96688742e-04 2.69606266e-01\n",
      " 4.96688742e-04 2.61560255e-01 1.98440641e-01 4.96688742e-04\n",
      " 4.97142892e-01 4.96688742e-04 2.37245605e-02 4.96688742e-04\n",
      " 3.49517332e-01 1.97462140e-01 4.40447318e-01 4.96688742e-04\n",
      " 1.77474629e-01 1.09302480e-01 4.96688742e-04 5.69965768e-01\n",
      " 4.96688742e-04 2.65873710e-01 2.63880569e-01 4.96688742e-04\n",
      " 1.42004420e-01 4.96688742e-04 4.96688742e-04 1.59652169e+00\n",
      " 4.96688742e-04 3.41888837e-01 4.96688742e-04 9.91596815e-02\n",
      " 4.96688742e-04 1.77660467e-01 4.96688742e-04 2.36422853e-01\n",
      " 4.96688742e-04 2.46891883e-02 4.96688742e-04 3.63721266e-01\n",
      " 1.57957968e-01 1.77377537e-02 2.68767676e-01 4.96688742e-04\n",
      " 2.01378114e-01 4.96688742e-04 3.06826392e-01 4.96688742e-04\n",
      " 1.77443936e-01 3.74171560e-01 4.96688742e-04 1.97824189e-01\n",
      " 4.96688742e-04 4.96688742e-04 4.96688742e-04 4.52426010e-01\n",
      " 4.96688742e-04 4.96688742e-04 8.50496689e-01 8.50496689e-01\n",
      " 8.50496689e-01 1.61549669e+00 4.96688742e-04 8.50496689e-01\n",
      " 4.96688742e-04 3.59338431e-01 4.96688742e-04 4.96688742e-04\n",
      " 7.86957781e-01 7.86957781e-01 7.86957781e-01 6.54358720e-01\n",
      " 4.96688742e-04 4.96688742e-04 7.81858720e-01 4.96688742e-04\n",
      " 7.23418874e-01 8.61683986e-02 6.96414839e-01 5.07461175e-01\n",
      " 6.96414839e-01 4.96688742e-04 2.50425965e-01 4.96688742e-04\n",
      " 4.96688742e-04 6.96414839e-01 7.35503698e-01 4.96688742e-04\n",
      " 9.98650298e-03 4.96688742e-04 4.96688742e-04 8.50496689e-01\n",
      " 4.96688742e-04 8.50496689e-01 7.21798233e-01 4.96688742e-04\n",
      " 8.50496689e-01 7.23418874e-01 7.23418874e-01 4.96688742e-04\n",
      " 4.96688742e-04 4.96688742e-04 4.96688742e-04 4.96688742e-04\n",
      " 4.96688742e-04 6.74503933e-01 4.96688742e-04 4.96688742e-04\n",
      " 7.23418874e-01 4.96688742e-04]\n"
     ]
    }
   ],
   "source": [
    "sentence_scores = calculate_pagerank_scores(sentence_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' richter , _ phys ',\n",
       " ' therefore , in the limit of the not frustrated ( @xmath136 ) square lattice ( @xmath67 ) antiferromagnetic , solving the equations ( 12 ) and applying the corrections factor we found @xmath146 which is consistent with the numerical results obtained by various methods such as series expansion , quantum monte carlo simulation , and others@xcite , and can also be compared with experimental results for the k@xmath24nif@xmath25 , k@xmath24mnf@xmath25 , and rb@xmath147mnf@xmath25 compounds@xcite ',\n",
       " 'these results are in accordance with results obtained by starykh and balentes@xcite , that predicted not the * qtp * in the ground - state phase diagram recently observed by bishop , _ _ et al',\n",
       " 'there are two magnetically long - range ordered phases at small and at large values of @xmath6 separated by an intermediate quantum paramagnetic phase without magnetic long - range order in the region between @xmath14 and @xmath15 , where the properties of these disordered phase are still under intensive debate ',\n",
       " 'et al__@xcite , by using coupled cluster treatment found the surprising and novel result that there exists a quantum triple point ( * qtp * ) with coordinates at ( @xmath43 ) , below which there is a second - order phase transition between the * af * and * caf * phases while above this * qtp * are these two ordered phases separated by the intermediate magnetically disordered phase ( vbs or rvb ) ']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_scores)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
