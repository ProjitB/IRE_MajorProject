{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_and_split(train_corpus_path):\n",
    "    \"\"\"\n",
    "    Read the full training data and split it into smaller chunks\n",
    "    :param train_corpus_path: input path\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_data_map = {}\n",
    "    file_no = 0\n",
    "    with open(train_corpus_path, 'r') as train_data_file:\n",
    "        line_count = 0\n",
    "        while file_no < 11:\n",
    "            if line_count < 20000:\n",
    "                line_data = train_data_file.readline()\n",
    "                if line_data:\n",
    "                    line_map = json.loads(line_data)\n",
    "                    article_id = line_map['article_id']\n",
    "                    del line_map['article_id']\n",
    "                    train_data_map[article_id] = line_map\n",
    "                    line_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "                    pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"File \", file_no, \" Done\")\n",
    "                train_data_map.clear()\n",
    "                file_no += 1\n",
    "                line_count = 0\n",
    "        with open(train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain/\" + \"train_\" + str(file_no) + \".pickle\", 'wb') as train_file:\n",
    "            pickle.dump(train_data_map, train_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_path = \"/media/kaushik/Studies/IIITH/3_ThirdSem/IRE/Major Project/arxiv-release/arxiv-release/train.txt\"\n",
    "# read_data_and_split(train_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = train_corpus_path.rsplit('/', 1)[0] + \"/SplitTrain\"\n",
    "data_map = {}\n",
    "with open(data_path + \"/\" + \"train_0.pickle\", 'rb') as handle:\n",
    "    data_map = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_metadata(data_map):\n",
    "    full_text = []\n",
    "    sentence_metadata = []\n",
    "    list_of_sentences = []\n",
    "    c = 0\n",
    "    for article_id, data in data_map.items():\n",
    "        if c == 50:\n",
    "            section_data = data['sections']\n",
    "            section_names = data['section_names']\n",
    "            for i, section in enumerate(section_data):\n",
    "                for line in section:\n",
    "                    split_line = line.split('.')\n",
    "                    for l in split_line:\n",
    "                        list_of_sentences.append(l)\n",
    "                        sentence_metadata.append(section_names[i])\n",
    "        c += 1\n",
    "    return list_of_sentences, sentence_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(word):\n",
    "    \"\"\"\n",
    "    Checks if word is ascii or not\n",
    "    :param word: token\n",
    "    :return: Boolean\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    try:\n",
    "        word = word.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        valid = False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens(sentence):\n",
    "    punc_map = {}\n",
    "    punc_map = punc_map.fromkeys('!\"\\'()*+,;<>[\\\\]^`{|}~:=%&_#?-$/', ' ')\n",
    "    table = str.maketrans(punc_map)\n",
    "    tokens = sentence.lower().translate(table).split()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and is_ascii(word) and '@' not in word and len(word) > 1]            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_sentences(list_of_sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = \" \".join(sentence)\n",
    "        processed_sentences.append(get_processed_tokens(sentence))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_of_common_word(sentence1, sentence2):\n",
    "    common_count = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2:\n",
    "            if s1 == s2:\n",
    "                common_count += 1\n",
    "    return common_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(sentence1, sentence2, metadata):\n",
    "    common_words = get_no_of_common_word(sentence1, sentence2)\n",
    "    \n",
    "    score = common_words\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(processed_sentences, metadata):\n",
    "    sentence_graph = np.zeros(shape=(len(processed_sentences), len(processed_sentences)))\n",
    "    for i in range(len(processed_sentences)):\n",
    "        for j in range(len(processed_sentences)):\n",
    "            sentence1 = processed_sentences[i]\n",
    "            sentence2 = processed_sentences[j]\n",
    "            if i == j:\n",
    "                sentence_graph[i][j] = 0\n",
    "            else:\n",
    "                sentence_graph[i][j] = scoring(sentence1, sentence2, metadata)\n",
    "    return sentence_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(sentence_graph):\n",
    "    scores = np.zeros(len(sentence_graph))\n",
    "    for i,sentence in enumerate(sentence_graph):\n",
    "        scores[i] = sum(sentence_graph[i])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences_and_make_summary(sentences, processed_sentences, sentence_graph, scores):\n",
    "    scores_indices = np.argsort(scores)\n",
    "    ordered_sentences = scores_indices[::-1]\n",
    "    print(scores)\n",
    "    print(len(sentences))\n",
    "    for i in range(3):\n",
    "        print(ordered_sentences[i], scores[ordered_sentences[i]])\n",
    "        print(sentences[ordered_sentences[i]])\n",
    "        print(processed_sentences[ordered_sentences[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123.   0.  48.   0. 149.   0.  58.   0. 259. 136.   0.  39.   0. 126.\n",
      "  17.   0. 278.   0.   6. 110.   0.  66.   0.   6.   0.   2. 189.   0.\n",
      "  83.  53.  16.   0.   0.   4.  77.   0.  15.   5.  63.   0. 127.   0.\n",
      "  22.   0.  82.  19.  46.   0.  24.   0.  44.   0.  46.   0. 199.   0.\n",
      "   5. 357.   0. 227.   0. 127.   0. 184.   0. 150.   0.   0.  77.   0.\n",
      "  41.   0.  12.   0.  13.   0.   4.   7.   0.  39.  77.  33.   0.  41.\n",
      "   0.  61.   0.  42.   0. 180.  24. 165.   2.  62.   0.   3.   0.  33.\n",
      "   3.   0.  39.   0.  16.   0.   1.   3.   0.   2.   0. 200.   0.  36.\n",
      "   0.  31.   0. 188. 106.   0.  63.  39. 281.   0. 142.   0.  75.   0.\n",
      "  55.   6.  68.   0. 158.   0.  26.  21. 145.   0.  71.  14.   0. 137.\n",
      "  21.   0. 217.   0. 203.   0.  44.   0.  60.   0.   0.   0.  25.  18.\n",
      "  51.   0.  38.   0.  16.   0.   0.   7. 125.   0.  45.   0.   0. 169.\n",
      "   0. 275. 180.   9.   0.  75.  55.  46.   0.  26.  50. 105.  22. 127.\n",
      "  92.   0. 200.   0.  94. 226.   0. 253.   0. 100.   0.  76.   0. 137.\n",
      "   0. 278.  80.   0. 106.   0.   4.   0. 231. 105.   5.   0.  80.   6.\n",
      "   0. 371.   0. 148. 238.   0.  61.   0.   0. 143.   0. 105.   0.  75.\n",
      "   0.  55.   0. 141.   0.  15.   0. 147.  91.   2. 284.   0. 292.   0.\n",
      " 132.   0. 207. 175.   0.  66.   0.   0.   0.   5.   0.   0.   2.   2.\n",
      "   2.   6.   0.   1.   0.   5.   0.   0.   2.   2.   2.   2.   0.   0.\n",
      "   4.   0.   1.   3.   2.   2.   2.   0.   5.   0.   0.   2.   4.   0.\n",
      "   1.   0.   0.   1.   0.   1.   5.   0.   1.   1.   1.   0.   0.   0.\n",
      "   0.   0.   0.   5.   0.   0.   1.   0.]\n",
      "302\n",
      "211 371.0\n",
      "we see that for @xmath129 there exists an intermediate region between the critical point @xmath130 at which @xmath131 for the * af * phase , characterizing a second - order transition , and the point @xmath132 at which the @xmath133 order parameter presents a discontinuity for the * caf * phase , characterizing a first - order transition \n",
      "['see', 'exists', 'intermediate', 'region', 'critical', 'point', 'af', 'phase', 'characterizing', 'second', 'order', 'transition', 'point', 'order', 'parameter', 'presents', 'discontinuity', 'caf', 'phase', 'characterizing', 'first', 'order', 'transition']\n",
      "57 357.0\n",
      "et al__@xcite , by using coupled cluster treatment found the surprising and novel result that there exists a quantum triple point ( * qtp * ) with coordinates at ( @xmath43 ) , below which there is a second - order phase transition between the * af * and * caf * phases while above this * qtp * are these two ordered phases separated by the intermediate magnetically disordered phase ( vbs or rvb ) \n",
      "['et', 'al', 'using', 'coupled', 'cluster', 'treatment', 'found', 'surprising', 'novel', 'result', 'exists', 'quantum', 'triple', 'point', 'qtp', 'coordinates', 'second', 'order', 'phase', 'transition', 'af', 'caf', 'phases', 'qtp', 'two', 'ordered', 'phases', 'separated', 'intermediate', 'magnetically', 'disordered', 'phase', 'vbs', 'rvb']\n",
      "236 292.0\n",
      "we have observed , by analyzing the order parameters of the * af * and * caf * phases , that the phase transitions are of second and first - order between the * af - qp * and * caf - qp * , respectively \n",
      "['observed', 'analyzing', 'order', 'parameters', 'af', 'caf', 'phases', 'phase', 'transitions', 'second', 'first', 'order', 'af', 'qp', 'caf', 'qp', 'respectively']\n"
     ]
    }
   ],
   "source": [
    "list_of_sentences, sentence_metadata = get_sentences_with_metadata(data_map)\n",
    "processed_sentences = make_processed_sentences(list_of_sentences)\n",
    "sentence_graph = make_graph(processed_sentences, sentence_metadata)\n",
    "sentence_scores = calculate_scores(sentence_graph)\n",
    "summary = rank_sentences_and_make_summary(list_of_sentences, processed_sentences, sentence_graph, sentence_scores)\n",
    "# summary = rank_sentences_and_make_summary(processed_sentences, sentence_graph, sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
